{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example IRIS code \n",
    "\n",
    "This example code computes the WSS for SWORD reach 22791100061 in the same way it is computed for the ICESat-2 River Surface Slope (IRIS)dataset.  \n",
    "The method is described in detail in https://doi.org/10.1029/2022WR032842.\n",
    "\n",
    "The input data for IRIS are:\n",
    "- ATL13: ATLAS/ICESat-2 L3A Along Track Inland Surface Water Data (*Jasinski et al. 2021*)\n",
    "- SWORD: SWOT Mission River Database (*Altenau et al. 2021*).\n",
    "\n",
    "##### References\n",
    "Altenau, E. H. et al. SWOT River Database (SWORD) (Version v1), https://doi.org/10.5281/zenodo.4917236 (2021).  \n",
    "Jasinski, M. et al. ATLAS/ICESat-2 L3A Inland Water Surface Height, Version 5., https://doi.org/10.5067/ATLAS/ATL13.005 (2021).  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IRIS data is calculated on reach scale using functions of the custom SWORD_Reach class.  \n",
    "Objects of the SWORD_Reach class are a python representation of the SWORD reach netcdf data provided by Altenau et al. (2021).  \n",
    "Note, that all of the functions below are part of the SWORD_Reach class and are only shown for clarity and possible alternation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dgfi_if import SWORD_Reach, Conversions, Utilities, SWORDException\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "tqdm.pandas()\n",
    "from shapely.geometry import Polygon, LineString\n",
    "from shapely.ops import unary_union, nearest_points\n",
    "from shapely import wkt\n",
    "import pickle, warnings\n",
    "from copy import deepcopy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IRIS code expects the ATL13 data to be in the DGFI-TUM's internal MVA format.  \n",
    "Since the MVA database can not be shared, this example script includes a SWORD_Reach object (**r**) for SWORD reach 22791100061 which already includes all the ATL13 data extracted from the MVA database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pickle.load(open('./22791100061.pickle','rb')) # load the object for SWORD reach 22791100061\n",
    "import pathlib\n",
    "r.nc_path = pathlib.Path(\"SWORD_Root/v2.0/Reaches_Nodes/netcdf/eu_sword_v2.nc\")\n",
    "r.sword_root = pathlib.Path(\"SWORD_Root/v2.0/Reaches_Nodes\")\n",
    "r.sword_base_root = pathlib.Path(\"SWORD_Root\")\n",
    "pickle.dump(r,open('./22791100061.pickle','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the many SWORD_Reach functions is ***get_aoi*** which buffers the reach's centerline by its average width to construct a polygon that defines the area of interest (AOI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"18.89232531985831 45.61114267552238 0.08510402257405403 0.11117833266025201\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,91.33346368370502)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.00222356665320504\" opacity=\"0.6\" d=\"M 18.906809066402193,45.6257543383408 L 18.907449457253673,45.62603148628967 L 18.90812922270032,45.62625781070619 L 18.90884020122619,45.62643059407931 L 18.909573856261204,45.626547761739126 L 18.92713709152192,45.628648844083166 L 18.939020089015074,45.64878853382532 L 18.93929325904606,45.649196272292485 L 18.93961953487472,45.64958433454347 L 18.93999606507094,45.64994932788532 L 18.94041955865952,45.65028806124144 L 18.940886313871523,45.6505975730631 L 18.95528893854694,45.65928846469706 L 18.95817288903343,45.662595761729364 L 18.959453124748276,45.66687824595055 L 18.958294212902917,45.671900941346756 L 18.955524248752017,45.67371818288278 L 18.93260712582718,45.681780345769624 L 18.93180017920799,45.68211417870851 L 18.916973368365973,45.68924349593351 L 18.916455617167525,45.689518464923154 L 18.915977201750955,45.68982645206961 L 18.915542393197597,45.69016470820758 L 18.915155073552178,45.690530213937244 L 18.90192397946877,45.70432939731931 L 18.901609102507397,45.70469108236927 L 18.901340783664203,45.70507074814834 L 18.90112109804183,45.70546545976876 L 18.89644303588276,45.71510040683475 L 18.90946725753764,45.71820329215818 L 18.913849633275216,45.709173172170175 L 18.925836862852528,45.69666874514611 L 18.93919277512803,45.69024624196038 L 18.96258036300774,45.68201813806513 L 18.963152983685372,45.68179194291388 L 18.963692161675255,45.6815284453492 L 18.964192967035682,45.6812300548609 L 18.969401745306392,45.67781240981957 L 18.969852113604563,45.67748772657279 L 18.97025700383128,45.67713483675592 L 18.97061283975035,45.67675685805155 L 18.970916478700723,45.67635712973521 L 18.971165239337395,45.67593918316159 L 18.971356925288706,45.67550671055582 L 18.97148984452209,45.67506353238553 L 18.97322549483557,45.66753351831258 L 18.97329739870639,45.66709446873079 L 18.97331162640791,45.666652646844575 L 18.973268059615638,45.66621176925826 L 18.973167066160713,45.6657755445462 L 18.97138019847666,45.659803155761644 L 18.97117592097257,45.659272827254064 L 18.97088689467101,45.658762248538736 L 18.97051687804183,45.65827805511636 L 18.96644988109565,45.65361518945322 L 18.966046646188282,45.653202587441825 L 18.96558459671888,45.65282152377023 L 18.965068712375427,45.65247610434127 L 18.95108869109341,45.644041387069414 L 18.93837640498305,45.62250309944574 L 18.938079343517337,45.62206373930996 L 18.937720687555704,45.621647626098174 L 18.937304091113855,45.62125899764454 L 18.93683379803328,45.62090181178056 L 18.936314598732935,45.6205797060462 L 18.935751781408275,45.620295960664684 L 18.935151078174815,45.620053465156765 L 18.934518606704685,45.619854688933714 L 18.93386080794956,45.61970165616752 L 18.93318438058307,45.61959592519325 L 18.91339159148372,45.61722866992011 L 18.909356634282727,45.61526039154683 L 18.901517461814574,45.62317292416845 L 18.906809066402193,45.6257543383408 z\" /></g></svg>",
      "text/plain": [
       "<POLYGON ((18.907 45.626, 18.907 45.626, 18.908 45.626, 18.909 45.626, 18.91...>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_aoi(self,buffersize=None, std_multiplicator = 1):\n",
    "    \"\"\" get a coarse aoi of the reach buffered by the given parameter or the width plus four times the width standard deviation\"\"\"\n",
    "    if buffersize is None:\n",
    "        buffersize = self.width + std_multiplicator * self.width_std\n",
    "       \n",
    "    centerline_wgs84 = LineString(self.coords[1:-1])\n",
    "    epsg_utm = Conversions.convert_wgs_to_utm_zone(geometry=centerline_wgs84)\n",
    "    centerline_utm = Conversions.convert_wgs_geometry_to_utm(centerline_wgs84,epsg_utm)\n",
    "\n",
    "    centerline_utm = centerline_utm.simplify(tolerance=100)\n",
    "    aoi_utm = centerline_utm.buffer(buffersize, cap_style=2)\n",
    "    aoi_wgs84 = Conversions.convert_utm_geometry_to_wgs(aoi_utm, epsg_utm)\n",
    "    if aoi_wgs84.geom_type != 'Polygon':\n",
    "        aoi_wgs84 = aoi_wgs84.convex_hull\n",
    "    return list(aoi_wgs84.exterior.coords)\n",
    "r.aoi = get_aoi(r, std_multiplicator=0) # For IRIS v1 the std_multiplicator is set to 0\n",
    "Polygon(r.aoi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ATL13 data from MVA is stored in the attribute *features_by_mission*.  \n",
    "Each feature contains parts of the ATL13 data from a specific beam and day intersecting the reach AOI stored as a pandas DataFrame.  \n",
    "Below, the ATL13 data of the first feature is shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>wgs_coord</th>\n",
       "      <th>alongtrack_distance</th>\n",
       "      <th>elev</th>\n",
       "      <th>depth</th>\n",
       "      <th>atl13_slope</th>\n",
       "      <th>jday</th>\n",
       "      <th>strong</th>\n",
       "      <th>geoh07</th>\n",
       "      <th>water_body_id</th>\n",
       "      <th>water_body_type</th>\n",
       "      <th>cloud_flag_asr_atl09</th>\n",
       "      <th>cloud_flag_atm_atl09</th>\n",
       "      <th>layer_flag_atl09</th>\n",
       "      <th>qf_cloud</th>\n",
       "      <th>qf_ice</th>\n",
       "      <th>snow_ice_atl09</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.941152</td>\n",
       "      <td>45.631690</td>\n",
       "      <td>(18.941152, 45.63169)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>124.902</td>\n",
       "      <td>11.125</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>6993.82575</td>\n",
       "      <td>None</td>\n",
       "      <td>44.807</td>\n",
       "      <td>2006069</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.941118</td>\n",
       "      <td>45.631931</td>\n",
       "      <td>(18.941118, 45.631930999999994)</td>\n",
       "      <td>26.958228</td>\n",
       "      <td>124.910</td>\n",
       "      <td>11.125</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>6993.82575</td>\n",
       "      <td>None</td>\n",
       "      <td>44.807</td>\n",
       "      <td>2006069</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.941083</td>\n",
       "      <td>45.632184</td>\n",
       "      <td>(18.941083, 45.632183999999995)</td>\n",
       "      <td>55.253504</td>\n",
       "      <td>124.885</td>\n",
       "      <td>11.125</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>6993.82575</td>\n",
       "      <td>None</td>\n",
       "      <td>44.807</td>\n",
       "      <td>2006069</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.941068</td>\n",
       "      <td>45.632292</td>\n",
       "      <td>(18.941067999999998, 45.632292)</td>\n",
       "      <td>67.332574</td>\n",
       "      <td>124.844</td>\n",
       "      <td>11.125</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>6993.82575</td>\n",
       "      <td>None</td>\n",
       "      <td>44.807</td>\n",
       "      <td>2006069</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.941061</td>\n",
       "      <td>45.632349</td>\n",
       "      <td>(18.941060999999998, 45.632349)</td>\n",
       "      <td>73.700747</td>\n",
       "      <td>124.816</td>\n",
       "      <td>11.125</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>6993.82575</td>\n",
       "      <td>None</td>\n",
       "      <td>44.807</td>\n",
       "      <td>2006069</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>18.932704</td>\n",
       "      <td>45.692577</td>\n",
       "      <td>(18.932703999999998, 45.692577)</td>\n",
       "      <td>6809.702158</td>\n",
       "      <td>125.127</td>\n",
       "      <td>14.995</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>6993.82576</td>\n",
       "      <td>None</td>\n",
       "      <td>44.772</td>\n",
       "      <td>2006069</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>18.932696</td>\n",
       "      <td>45.692634</td>\n",
       "      <td>(18.932696, 45.692634)</td>\n",
       "      <td>6816.077786</td>\n",
       "      <td>125.133</td>\n",
       "      <td>14.995</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>6993.82576</td>\n",
       "      <td>None</td>\n",
       "      <td>44.772</td>\n",
       "      <td>2006069</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>18.932688</td>\n",
       "      <td>45.692698</td>\n",
       "      <td>(18.932688, 45.692698)</td>\n",
       "      <td>6823.229007</td>\n",
       "      <td>125.149</td>\n",
       "      <td>14.995</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>6993.82576</td>\n",
       "      <td>None</td>\n",
       "      <td>44.772</td>\n",
       "      <td>2006069</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>18.932681</td>\n",
       "      <td>45.692749</td>\n",
       "      <td>(18.932681, 45.692749)</td>\n",
       "      <td>6828.932330</td>\n",
       "      <td>125.168</td>\n",
       "      <td>14.995</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>6993.82576</td>\n",
       "      <td>None</td>\n",
       "      <td>44.772</td>\n",
       "      <td>2006069</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>18.932667</td>\n",
       "      <td>45.692858</td>\n",
       "      <td>(18.932667, 45.692858)</td>\n",
       "      <td>6841.114571</td>\n",
       "      <td>125.189</td>\n",
       "      <td>14.995</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>6993.82576</td>\n",
       "      <td>None</td>\n",
       "      <td>44.772</td>\n",
       "      <td>2006069</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lon        lat                        wgs_coord  \\\n",
       "0    18.941152  45.631690            (18.941152, 45.63169)   \n",
       "1    18.941118  45.631931  (18.941118, 45.631930999999994)   \n",
       "2    18.941083  45.632184  (18.941083, 45.632183999999995)   \n",
       "3    18.941068  45.632292  (18.941067999999998, 45.632292)   \n",
       "4    18.941061  45.632349  (18.941060999999998, 45.632349)   \n",
       "..         ...        ...                              ...   \n",
       "465  18.932704  45.692577  (18.932703999999998, 45.692577)   \n",
       "466  18.932696  45.692634           (18.932696, 45.692634)   \n",
       "467  18.932688  45.692698           (18.932688, 45.692698)   \n",
       "468  18.932681  45.692749           (18.932681, 45.692749)   \n",
       "469  18.932667  45.692858           (18.932667, 45.692858)   \n",
       "\n",
       "     alongtrack_distance     elev   depth  atl13_slope        jday strong  \\\n",
       "0               0.000000  124.902  11.125    -0.000031  6993.82575   None   \n",
       "1              26.958228  124.910  11.125    -0.000031  6993.82575   None   \n",
       "2              55.253504  124.885  11.125    -0.000031  6993.82575   None   \n",
       "3              67.332574  124.844  11.125    -0.000031  6993.82575   None   \n",
       "4              73.700747  124.816  11.125    -0.000031  6993.82575   None   \n",
       "..                   ...      ...     ...          ...         ...    ...   \n",
       "465          6809.702158  125.127  14.995     0.000660  6993.82576   None   \n",
       "466          6816.077786  125.133  14.995     0.000660  6993.82576   None   \n",
       "467          6823.229007  125.149  14.995     0.000660  6993.82576   None   \n",
       "468          6828.932330  125.168  14.995     0.000660  6993.82576   None   \n",
       "469          6841.114571  125.189  14.995     0.000660  6993.82576   None   \n",
       "\n",
       "     geoh07  water_body_id  water_body_type  cloud_flag_asr_atl09  \\\n",
       "0    44.807        2006069                5                     2   \n",
       "1    44.807        2006069                5                     2   \n",
       "2    44.807        2006069                5                     2   \n",
       "3    44.807        2006069                5                     2   \n",
       "4    44.807        2006069                5                     2   \n",
       "..      ...            ...              ...                   ...   \n",
       "465  44.772        2006069                5                     1   \n",
       "466  44.772        2006069                5                     1   \n",
       "467  44.772        2006069                5                     1   \n",
       "468  44.772        2006069                5                     1   \n",
       "469  44.772        2006069                5                     1   \n",
       "\n",
       "     cloud_flag_atm_atl09  layer_flag_atl09  qf_cloud  qf_ice  snow_ice_atl09  \n",
       "0                       1                 0       NaN     NaN               1  \n",
       "1                       1                 0       NaN     NaN               1  \n",
       "2                       1                 0       NaN     NaN               1  \n",
       "3                       1                 0       NaN     NaN               1  \n",
       "4                       1                 0       NaN     NaN               1  \n",
       "..                    ...               ...       ...     ...             ...  \n",
       "465                     1                 0       NaN     NaN               1  \n",
       "466                     1                 0       NaN     NaN               1  \n",
       "467                     1                 0       NaN     NaN               1  \n",
       "468                     1                 0       NaN     NaN               1  \n",
       "469                     1                 0       NaN     NaN               1  \n",
       "\n",
       "[470 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.features_by_mission['icesat2_gt1l_atl13v5_hf'][0]['data']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we iterate over all features and apply the function *icesat2_pass_helper*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    }
   ],
   "source": [
    "def icesat2_pass_helper(self, feature, max_median_deviation : float = 0.05, min_size_to_apply_window : int = 20, window_size : int = 7, debug = False):\n",
    "    \"\"\"Helper Function to reject outliers and convert along track slope to along centerline slope\"\"\"\n",
    "    pass_geom = wkt.loads(feature['geometry'])\n",
    "    centerline = LineString(self.coords)\n",
    "\n",
    "    #######################################################################################\n",
    "    ### First Intersection for splitting features if necessary\n",
    "    intersection = pass_geom.intersection(centerline)\n",
    "    if intersection.is_empty:\n",
    "        nearest = nearest_points(pass_geom, centerline)\n",
    "        ref_point_lon = nearest[0].x\n",
    "        ref_point_lat = nearest[0].y\n",
    "    elif intersection.geom_type == 'Point':\n",
    "        ref_point_lon = intersection.x\n",
    "        ref_point_lat = intersection.y\n",
    "    elif intersection.geom_type == 'MultiPoint':\n",
    "        distances = {}\n",
    "        for i, point in enumerate(intersection.geoms):\n",
    "            distances[i] = feature['data'].apply(lambda x: Utilities.openadb_spherical_distance(point.x,point.y,x.lon,x.lat),axis=1)\n",
    "        df = pd.DataFrame(distances)\n",
    "        nearest_point = df.idxmin(axis=1)\n",
    "        data = []\n",
    "        for i in distances.keys():\n",
    "            this_index = nearest_point[nearest_point == i]\n",
    "            new_feature = deepcopy(feature)\n",
    "            new_feature['data'] = new_feature['data'][new_feature['data'].index.isin(this_index.index)]\n",
    "            if len(new_feature['data'].lon) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                new_feature['geometry'] = Utilities.linestring_helper(new_feature['data'].lon, new_feature['data'].lat)\n",
    "                _data = icesat2_pass_helper(self,feature=new_feature,max_median_deviation=max_median_deviation,min_size_to_apply_window=min_size_to_apply_window,window_size=window_size,debug=debug)\n",
    "            except Exception as e:\n",
    "                _data = None\n",
    "            if _data is not None:\n",
    "                data.append(_data)\n",
    "        return data\n",
    "    #######################################################################################\n",
    "\n",
    "    atl13_pass_data = feature['data']\n",
    "    epsg_utm = Utilities.convert_wgs_to_utm_zone(lon=ref_point_lon, lat=ref_point_lat)\n",
    "    ydata = atl13_pass_data.elev - atl13_pass_data.geoh07\n",
    "    xdata = atl13_pass_data.alongtrack_distance\n",
    "\n",
    "    #######################################################################################\n",
    "    ### Outlier Detection\n",
    "\n",
    "    if ydata.shape[0] > min_size_to_apply_window:\n",
    "        window_median = ydata.rolling(window_size,min_periods=int(np.floor(window_size/2)),center=True).median()\n",
    "        median_flags = (window_median - ydata).abs() <= max_median_deviation # False : AMD Outlier\n",
    "    else:\n",
    "        median_flags = (ydata - ydata.median()) <= max_median_deviation # False : AMD Outlier\n",
    "    \n",
    "    type_flags = True #atl13_pass_data.water_body_type.isin([2,5,6]) # Allowed Values: Reservoir, River, Estuary -> False : Type Outlier\n",
    "    cloud_flags = atl13_pass_data.cloud_flag_asr_atl09.isin([0,1,2,3]) # -> False: Cloud Outlier\n",
    "    ice_flags = atl13_pass_data.snow_ice_atl09.isin([0,1]) # -> False: Ice Outlier\n",
    "    flags = type_flags & median_flags & cloud_flags & ice_flags # False : Type, Cloud, Ice, and AMD Outlier\n",
    "    \n",
    "    atl13_pass_data.loc[:,\"type_flag\"] = type_flags\n",
    "    atl13_pass_data.loc[:,\"median_flag\"] = median_flags\n",
    "    atl13_pass_data.loc[:,\"cloud_flag\"] = cloud_flags\n",
    "    atl13_pass_data.loc[:,\"ice_flags\"] = ice_flags\n",
    "    \n",
    "    xdata_ice_outlier = xdata.loc[~ice_flags]\n",
    "    ydata_ice_outlier = ydata.loc[~ice_flags]\n",
    "\n",
    "    amd_xdata_outlier = xdata.loc[~flags] # For Debug Plot\n",
    "    amd_ydata_outlier = ydata.loc[~flags] # For Debug Plot\n",
    "    xdata = xdata.loc[flags]\n",
    "    ydata = ydata.loc[flags]\n",
    "    \n",
    "    if xdata.shape[0] < 2:\n",
    "        'No Data after AMD and Type Outlier removal'\n",
    "        return None\n",
    "    \n",
    "    clusters = (xdata.diff() > 500).cumsum()\n",
    "    cluster_flags = clusters == clusters.value_counts().idxmax() # False: Cluster Outlier (Only longest Cluster is used)\n",
    "\n",
    "    cluster_xdata_outlier = xdata.loc[~cluster_flags] # For Debug Plot\n",
    "    cluster_ydata_outlier = ydata.loc[~cluster_flags] # For Debug Plot\n",
    "    xdata = xdata.loc[cluster_flags]\n",
    "    ydata = ydata.loc[cluster_flags]\n",
    "\n",
    "    atl13_pass_data.loc[:,\"cluster_flag\"] = atl13_pass_data.index.isin(xdata.index) #cluster_flags\n",
    "\n",
    "    if xdata.shape[0] < 2:\n",
    "        'No Data after Cluster Outlier removal'\n",
    "        return None\n",
    "\n",
    "    #######################################################################################\n",
    "    ###  Position and Angle Determination\n",
    "    pass_geom = Utilities.validate_geometry(LineString(atl13_pass_data[atl13_pass_data.cluster_flag.fillna(False)].wgs_coord.to_list()))\n",
    "    intersection = pass_geom.intersection(centerline)\n",
    "    if intersection.is_empty:\n",
    "        nearest = nearest_points(pass_geom, centerline)\n",
    "        ref_point_lon = nearest[0].x\n",
    "        ref_point_lat = nearest[0].y\n",
    "    elif intersection.geom_type == 'Point':\n",
    "        ref_point_lon = intersection.x\n",
    "        ref_point_lat = intersection.y\n",
    "    elif intersection.geom_type == 'MultiPoint':\n",
    "        raise SWORDException('Feature should be split')\n",
    "    atl13_pass_data.loc[:,'ref_distance'] = Utilities.great_circle_dist(atl13_pass_data.lon.to_numpy(), atl13_pass_data.lat.to_numpy(), ref_point_lon, ref_point_lat) * 1000\n",
    "    x0 = atl13_pass_data.alongtrack_distance.loc[atl13_pass_data.ref_distance.idxmin()]\n",
    "    postion = self.get_reach_position_of_closest_cl_point(ref_point_lon, ref_point_lat)\n",
    "    node_id = self.search_reach_id(lon=ref_point_lon, lat=ref_point_lat, return_node=True)\n",
    "    normal = self.get_node_normal(node_id, pointing='straight')\n",
    "    normal_utm = Utilities.convert_wgs_geometry_to_utm(normal, epsg_utm)\n",
    "    normal_coords = np.asarray(Utilities.get_geom_coordinates(normal_utm))\n",
    "    pass_geom_utm = Utilities.convert_wgs_geometry_to_utm(pass_geom, epsg_utm)\n",
    "    pass_coords = np.asarray(Utilities.get_geom_coordinates(pass_geom_utm))\n",
    "    \n",
    "    normal_vector = normal_coords[-1] - normal_coords[0]\n",
    "    pass_vector = pass_coords[-1] - pass_coords[0]\n",
    "    u, v = pass_vector, normal_vector\n",
    "    u_norm = np.sqrt(sum(u*u))\n",
    "    v_norm = np.sqrt(sum(v*v))\n",
    "    dot = np.dot(u, v)\n",
    "\n",
    "    #######################################################################################\n",
    "    ### This Block handles pass height estmate using SVR based on DAHITI approach and SVR outlier rejection\n",
    "    limit = .05\n",
    "    distances = np.abs(xdata - x0)\n",
    "    distance_weights = 1./np.where(distances==0,1,distances)\n",
    "    xscaler = StandardScaler() # SVR works better when data is scaled\n",
    "    yscaler = StandardScaler()\n",
    "    xscaler.fit(xdata.to_numpy().reshape(-1, 1))\n",
    "    yscaler.fit(ydata.to_numpy().reshape(-1, 1))\n",
    "    xscaled = xscaler.transform(xdata.to_numpy().reshape(-1, 1))\n",
    "    yscaled = yscaler.transform(ydata.to_numpy().reshape(-1, 1))\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(action='ignore',message='Liblinear failed to converge, increase the number of iterations.')\n",
    "        svr = LinearSVR(random_state=0,max_iter=1e6)\n",
    "        svr.fit(xscaled,np.ravel(yscaled),sample_weight=distance_weights)\n",
    "    y_predict = yscaler.inverse_transform(svr.predict(xscaled))\n",
    "    y_deviation = (ydata - y_predict).abs()\n",
    "    svr_flags = y_deviation <= limit\n",
    "    y_predict = y_predict[svr_flags]\n",
    "    if y_predict.size > 0:\n",
    "        svr_elev = np.average(ydata[svr_flags],weights=distance_weights[svr_flags])\n",
    "        median_elev = np.median(ydata)\n",
    "    else:\n",
    "        svr_elev = np.nan\n",
    "        median_elev = np.nan\n",
    "\n",
    "    svr_xdata_outlier = xdata.loc[~svr_flags] # For Debug Plot\n",
    "    svr_ydata_outlier = ydata.loc[~svr_flags] # For Debug Plot\n",
    "    xdata = xdata[svr_flags]\n",
    "    ydata = ydata[svr_flags]\n",
    "    width = np.max(xdata) - np.min(xdata)\n",
    "    atl13_pass_data.loc[:,'svr_flag'] = atl13_pass_data.index.isin(xdata.index)\n",
    "    atl13_pass_data.loc[:,'rejected'] = ~(atl13_pass_data.svr_flag & atl13_pass_data.cluster_flag & atl13_pass_data.median_flag & atl13_pass_data.type_flag)\n",
    "\n",
    "    if xdata.shape[0] < 2:\n",
    "        'No Data after SVR Outlier removal'\n",
    "        return None\n",
    "\n",
    "    #######################################################################################\n",
    "    ### This Block handles the conversion from atl13 along track slope to along river centerline slope\n",
    "    dot_crossing_angle_deg = np.rad2deg(np.arccos(dot/(u_norm * v_norm)))\n",
    "    if dot != 0 and xdata.size > 1: # orthogonal flying\n",
    "        tinv = lambda p, df: abs(stats.t.ppf(p/2, df))\n",
    "        ts = tinv(0.05, len(xdata)-2)\n",
    "        slope, intercept, r, p, se = stats.linregress(xdata, ydata)\n",
    "        slope_confidence = ts*se \n",
    "        dh = -1 * (u_norm * slope)\n",
    "        proj_of_u_on_v = (dot/v_norm**2)*v\n",
    "        l = np.sqrt(sum(proj_of_u_on_v**2))\n",
    "        adjusted_custom_along_track_slope = (dh/l) * np.sign(dot)\n",
    "    else:\n",
    "        adjusted_custom_along_track_slope = np.nan\n",
    "        slope, intercept, r, p, se = np.nan,np.nan,np.nan,np.nan,np.nan\n",
    "        slope_confidence = np.nan\n",
    "    \n",
    "    #######################################################################################\n",
    "\n",
    "    date_object = Conversions.julianDayDate(np.nanmean(atl13_pass_data.jday))\n",
    "    date = datetime.datetime(year=date_object[\"year\"], month=date_object[\"month\"], day=date_object[\"day\"], hour=date_object[\"hour\"], minute=date_object[\"minute\"])\n",
    "            \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(action='ignore')\n",
    "        return {\n",
    "            'date': date.date(),\n",
    "            'timestamp': date,\n",
    "            'mission': feature['mission'],\n",
    "            'position': postion,\n",
    "            'lon': ref_point_lon,\n",
    "            'lat': ref_point_lat,\n",
    "            'median_elev': median_elev,\n",
    "            'svr_elev' : svr_elev,\n",
    "            'elev' : svr_elev if svr_elev is not None else median_elev,\n",
    "            'elev_std': atl13_pass_data.elev[~atl13_pass_data.rejected].std(),\n",
    "            'atl13_slope': atl13_pass_data.atl13_slope[~atl13_pass_data.rejected].median(),\n",
    "            'min_depth': atl13_pass_data.depth[~atl13_pass_data.rejected].min(),\n",
    "            'max_depth': atl13_pass_data.depth[~atl13_pass_data.rejected].max(),\n",
    "            'median_depth': atl13_pass_data.depth[~atl13_pass_data.rejected].median(),\n",
    "            'water_body_ids' : atl13_pass_data.water_body_id[~atl13_pass_data.rejected].unique(),\n",
    "            'water_body_types' : atl13_pass_data.water_body_type[~atl13_pass_data.rejected].unique(),\n",
    "            'hf_data': atl13_pass_data,\n",
    "            'custom_along_track_slope' : slope,\n",
    "            'custom_along_track_slope_err': se,\n",
    "            'custom_along_track_slope_abs_rvalue': np.abs(r),\n",
    "            'custom_along_track_slope_pvalue': p,\n",
    "            'adjusted_custom_along_track_slope' : adjusted_custom_along_track_slope,\n",
    "            'dot_crossing_angle_deg' : dot_crossing_angle_deg,\n",
    "            'node_id' : node_id,\n",
    "            'width' : width,\n",
    "            'slope_confidence': slope_confidence,\n",
    "            }\n",
    "\n",
    "icesat2_data = []\n",
    "def rec_append(items):\n",
    "    # Helper Function for nested data\n",
    "    if isinstance(items,list):\n",
    "        for item in items:\n",
    "            rec_append(item)\n",
    "    else:\n",
    "        icesat2_data.append(items)\n",
    "\n",
    "for features in tqdm(list(r.features_by_mission.values()), desc=f'Estimating ICESat-2 Heights and Along Slope for Reach {r.reach_id} by beam', leave=False):\n",
    "    for feature in features:\n",
    "        if feature['data'].empty:\n",
    "            continue\n",
    "        data = icesat2_pass_helper(r, feature)\n",
    "        if data is None:\n",
    "            continue\n",
    "        if isinstance(data, list):\n",
    "            rec_append(data)\n",
    "        else:\n",
    "            icesat2_data.append(data)\n",
    "r.icesat2_data = pd.DataFrame(icesat2_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above block rejected outliers and created the r.icesat2_data attribute containing all features (independent intersections with the river).  \n",
    "It also computed the along-track slope, its confidence interval, and the feature positions and median elevations required for the across track approach.\n",
    "\n",
    "Now, using the *icesat2_slope_helper* below, the final along-track and across-track slopes are calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icesat2_slope_helper(self, min_distance : float, along_angle_th = None, along_conf_th = None):\n",
    "    mean_across_slope_by_date = {}\n",
    "    mean_across_std_by_date = {}\n",
    "    across_slopes_by_date = {}\n",
    "    across_distances_by_date = {}\n",
    "    mean_along_slope_by_date = {}\n",
    "    across_x_angles_by_date = {}\n",
    "    across_y_angles_by_date = {}\n",
    "    mean_along_angle_by_date = {}\n",
    "    along_slopes_by_date = {}\n",
    "    water_body_types_by_date = {}\n",
    "    water_body_ids_by_date = {}\n",
    "    date_groups = dict(list(self.icesat2_data.groupby('date')))\n",
    "    for date, df in date_groups.items():\n",
    "        if len(df.index) > 1:\n",
    "            ########### Across Slope\n",
    "            cols = ['mission','position','elev','median_elev','elev_std','dot_crossing_angle_deg']\n",
    "            cross = df[cols].merge(df[cols], how='cross')\n",
    "            ### absolute distance only to remove duplicates to too close observations:\n",
    "            cross['distance'] = np.abs(cross.position_x - cross.position_y)\n",
    "            cross = cross[cross['distance'] > min_distance]\n",
    "            cross = cross.drop_duplicates(subset=\"distance\").reset_index(drop=True)\n",
    "            if not cross.empty:\n",
    "                ###\n",
    "                ### Actual distance\n",
    "                cross['distance'] = cross.position_x - cross.position_y\n",
    "                cross['delta_h'] = cross.elev_x - cross.elev_y\n",
    "                cross['slope'] = cross.delta_h / cross.distance\n",
    "                cross['weight'] = 1./(cross.elev_std_x + cross.elev_std_y)\n",
    "                cross = cross[cross['slope'] >= 0]\n",
    "                try:\n",
    "                    mean_across_slope_by_date[date] = np.average(cross.slope,weights=cross.weight)\n",
    "                    mean_across_std_by_date[date] = np.average((cross.elev_std_x + cross.elev_std_y),weights=cross.weight)\n",
    "                except ZeroDivisionError:\n",
    "                    mean_across_slope_by_date[date] = np.nanmean(cross.slope)\n",
    "                    mean_across_std_by_date[date] = np.nanmean((cross.elev_std_x + cross.elev_std_y))\n",
    "                across_slopes_by_date[date] = cross['slope']\n",
    "                across_distances_by_date[date] = cross['distance']\n",
    "                across_x_angles_by_date[date] = cross['dot_crossing_angle_deg_x']\n",
    "                across_y_angles_by_date[date] = cross['dot_crossing_angle_deg_y']\n",
    "        if len(df.index) > 0:\n",
    "            ########### Along Slope\n",
    "            def is_unique(s):\n",
    "                a = s.values\n",
    "                return (a[0] == a).all()\n",
    "            \n",
    "            if along_conf_th is not None and along_angle_th is not None:\n",
    "                df.loc[:,'conf_threshold'] = df['dot_crossing_angle_deg'].apply(Utilities.icesat2_along_get_conf_threshold_by_angle, args=(along_conf_th, along_angle_th))\n",
    "                df = df[df.slope_confidence * 1e6 < df.conf_threshold]\n",
    "                if df.empty:\n",
    "                    continue\n",
    "            \n",
    "            p_index = df.adjusted_custom_along_track_slope >= 0\n",
    "            df.loc[:,'reduced_angle_deg'] = df.dot_crossing_angle_deg.apply(lambda x: 180-x if x > 90 else x)\n",
    "            if np.sum(p_index) > 0:\n",
    "                along_slopes_by_date[date] = df.adjusted_custom_along_track_slope\n",
    "                water_body_types_by_date[date] = is_unique(df.water_body_types.apply(pd.Series).stack().reset_index(drop=True))\n",
    "                water_body_ids_by_date[date] = is_unique(df.water_body_ids.apply(pd.Series).stack().reset_index(drop=True))\n",
    "                try:\n",
    "                    mean_along_slope_by_date[date] = np.average(df.adjusted_custom_along_track_slope[p_index], weights=1./df.reduced_angle_deg[p_index])\n",
    "                    mean_along_angle_by_date[date] = np.average(df.dot_crossing_angle_deg[p_index], weights=1./df.reduced_angle_deg[p_index])\n",
    "                except ZeroDivisionError:\n",
    "                    mean_along_slope_by_date[date] = np.nanmean(df.adjusted_custom_along_track_slope[p_index])\n",
    "                    mean_along_angle_by_date[date] = np.nanmean(df.dot_crossing_angle_deg[p_index])\n",
    "            \n",
    "    return pd.DataFrame({\n",
    "        \"mean_across\" : pd.Series(mean_across_slope_by_date),\n",
    "        \"mean_along\" : pd.Series(mean_along_slope_by_date),\n",
    "        \"across\" : pd.Series(across_slopes_by_date),\n",
    "        \"along\" : pd.Series(along_slopes_by_date),\n",
    "        \"mean_across_std\" : pd.Series(mean_across_std_by_date),\n",
    "        \"mean_along_angle\" : pd.Series(mean_along_angle_by_date),\n",
    "        \"across_distances\" : pd.Series(across_distances_by_date),\n",
    "        \"across_angle_x\" : pd.Series(across_x_angles_by_date),\n",
    "        \"across_angle_y\" : pd.Series(across_y_angles_by_date),\n",
    "        \"unique_water_body_types\" : pd.Series(water_body_types_by_date),\n",
    "        \"unique_water_body_ids\" : pd.Series(water_body_ids_by_date),\n",
    "    })\n",
    "\n",
    "def get_icesat2_slope(self, mva_instance = None, ee_instance = None, min_distance = 1000., debug=False, extend_aoi=False, extend_aoi_on_empty=False, silent=False, along_angle_th = None, along_conf_th = None):\n",
    "        \"\"\"\n",
    "        Convenience function to extract ICESat2 Data from MVA and calculate water surfac slope.\n",
    "        Sets the icesat2_data, icesat2_slope_data, mean_mixed_slope, mean_across_slope, and mean_along_slope attributes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mva_instance : Instance of dgfi_if.MVA_IF Class\n",
    "            Required in case the passes have to be intersected and extracted from mva.\n",
    "\n",
    "        ee_instance : Instance of dgfi_if.EE_IF Class, optional\n",
    "            In case the passes have to be intersected and extracted from mva:  \n",
    "            If given, the JRC Water Occurence > 80 will be used as AOI.  \n",
    "            Otherwise the standard SWORD AOI will be used (default is None)\n",
    "\n",
    "        min_distance : float\n",
    "            Minimum Distance betwen across beams to calculate across slope (default is 1000.)\n",
    "\n",
    "        debug : bool\n",
    "            prints debug messages and plots (default is False)\n",
    "        \n",
    "        extend_aoi : bool\n",
    "            Extends AOI to upstream and downstream reaches (default is False)\n",
    "\n",
    "        extend_aoi_on_empty : bool\n",
    "            Extends AOI to upstream and downstream reaches if there is no data for the instance reach (default is False)\n",
    "\n",
    "        silent : bool\n",
    "            Suppresses any output (default is False)\n",
    "\n",
    "        along_angle_th : float, optional\n",
    "            Maximum angle argument for the along track outlier rejection (Recommended is 65, default is None)\n",
    "\n",
    "        along_conf_th : float, optional\n",
    "            Maximum slope fit confidence interval argument for the along track outlier rejection (Recommended is 300, default is None)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if self.icesat2_data.empty:\n",
    "            self.icesat2_slope_data = pd.DataFrame()\n",
    "        else:\n",
    "            self.icesat2_slope_data = icesat2_slope_helper(r,min_distance = min_distance, along_angle_th = along_angle_th, along_conf_th = along_conf_th)\n",
    "            count = self.icesat2_slope_data.count()\n",
    "\n",
    "        if \"mean_along\" in self.icesat2_slope_data:\n",
    "            try:\n",
    "                self.mean_along_slope = self.icesat2_slope_data.mean_along.median()\n",
    "            except Exception as e:\n",
    "                # logger.warning('Cannot get along slope for reach {self.reach_id}')\n",
    "                # logger.exception(e, exc_info=True)\n",
    "                self.mean_along_slope = np.nan\n",
    "        else:\n",
    "            self.mean_along_slope = np.nan\n",
    "        if \"mean_across\" in self.icesat2_slope_data:\n",
    "            try:\n",
    "                self.mean_across_slope = np.average(self.icesat2_slope_data.mean_across.dropna(),weights=1./self.icesat2_slope_data.mean_across_std.dropna())\n",
    "            except ZeroDivisionError:\n",
    "                self.mean_across_slope = np.nanmean(self.icesat2_slope_data.mean_across.dropna())\n",
    "            except Exception:\n",
    "                print('Cannot get across slope for reach {self.reach_id}')\n",
    "                # logger.exception(e, exc_info=True)\n",
    "                self.mean_across_slope = np.nan\n",
    "        else:\n",
    "            self.mean_across_slope = np.nan\n",
    "        if \"mean_along\" in self.icesat2_slope_data and \"mean_across\" in self.icesat2_slope_data:\n",
    "            self.mean_mixed_slope = self.icesat2_slope_data.mean_across.dropna().combine_first(self.icesat2_slope_data.mean_along.dropna()).median()\n",
    "        else:\n",
    "            self.mean_mixed_slope = np.nanmean([self.mean_across_slope, self.mean_along_slope])\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "    get_icesat2_slope(r,along_angle_th=65,along_conf_th=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reach_id 22791100061\n",
      "lon 18.93469800034848\n",
      "lat 45.667934381764546\n",
      "across_flag True\n",
      "along_flag True\n",
      "combined_flag True\n",
      "avg_across_slope 45.46945176255353\n",
      "avg_along_slope 51.13468066437649\n",
      "avg_combined_slope 46.15083558441717\n",
      "min_across_slope 35.411036739409425\n",
      "min_along_slope 12.007751614902137\n",
      "min_combined_slope 12.007751614902137\n",
      "max_across_slope 52.39182837868987\n",
      "max_along_slope 109.57663399802425\n",
      "max_combined_slope 109.57663399802425\n",
      "std_across_slope 5.864640672545181\n",
      "std_along_slope 35.13865530340808\n",
      "std_combined_slope 23.25847476120308\n",
      "n_across_slope 11\n",
      "n_along_slope 11\n",
      "n_combined_slope 11\n",
      "min_date_across_slope 2019-02-24\n",
      "min_date_along_slope 2019-02-24\n",
      "min_date_combined_slope 2019-02-24\n",
      "max_date_across_slope 2022-05-19\n",
      "max_date_along_slope 2022-05-19\n",
      "max_date_combined_slope 2022-05-19\n"
     ]
    }
   ],
   "source": [
    "if \"mean_along\" in r.icesat2_slope_data and \"mean_across\" in r.icesat2_slope_data:\n",
    "    mixed_slope_series = r.icesat2_slope_data.mean_across.dropna().combine_first(r.icesat2_slope_data.mean_along.dropna())\n",
    "elif 'mean_along' in r.icesat2_slope_data:\n",
    "    mixed_slope_series = r.icesat2_slope_data.mean_along\n",
    "elif 'mean_across' in r.icesat2_slope_data:\n",
    "    mixed_slope_series = r.icesat2_slope_data.mean_across\n",
    "else:\n",
    "    mixed_slope_series = None\n",
    "\n",
    "print('reach_id' , r.reach_id)\n",
    "print('lon', r.simple_center[0])\n",
    "print('lat' , r.simple_center[1])\n",
    "print('across_flag' , False if np.isnan(r.mean_across_slope) else True)\n",
    "print('along_flag' , False if np.isnan(r.mean_along_slope) else True)\n",
    "print('combined_flag' , False if np.isnan(r.mean_mixed_slope) else True)\n",
    "print('avg_across_slope' , r.mean_across_slope * 1e6)\n",
    "print('avg_along_slope' , r.mean_along_slope * 1e6)\n",
    "print('avg_combined_slope' , r.mean_mixed_slope * 1e6)\n",
    "print('min_across_slope' , r.icesat2_slope_data.mean_across.min() * 1e6 if 'mean_across' in r.icesat2_slope_data else np.nan)\n",
    "print('min_along_slope' , r.icesat2_slope_data.mean_along.min() * 1e6 if 'mean_along' in r.icesat2_slope_data else np.nan)\n",
    "print('min_combined_slope' , mixed_slope_series.min() * 1e6 if mixed_slope_series is not None else np.nan)\n",
    "print('max_across_slope' , r.icesat2_slope_data.mean_across.max() * 1e6 if 'mean_across' in r.icesat2_slope_data else np.nan)\n",
    "print('max_along_slope' , r.icesat2_slope_data.mean_along.max() * 1e6 if 'mean_along' in r.icesat2_slope_data else np.nan)\n",
    "print('max_combined_slope' , mixed_slope_series.max() * 1e6 if mixed_slope_series is not None else np.nan)\n",
    "print('std_across_slope' , r.icesat2_slope_data.mean_across.std() * 1e6 if 'mean_across' in r.icesat2_slope_data else np.nan)\n",
    "print('std_along_slope' , r.icesat2_slope_data.mean_along.std() * 1e6 if 'mean_along' in r.icesat2_slope_data else np.nan)\n",
    "print('std_combined_slope' , mixed_slope_series.std() * 1e6 if mixed_slope_series is not None else np.nan)\n",
    "print('n_across_slope' , r.icesat2_slope_data.mean_across.shape[0] if 'mean_across' in r.icesat2_slope_data else 0)\n",
    "print('n_along_slope' , r.icesat2_slope_data.mean_along.shape[0] if 'mean_along' in r.icesat2_slope_data else 0)\n",
    "print('n_combined_slope' , mixed_slope_series.shape[0] if mixed_slope_series is not None else 0)\n",
    "print('min_date_across_slope' , r.icesat2_slope_data.mean_across.dropna().index.min() if 'mean_across' in r.icesat2_slope_data else np.nan)\n",
    "print('min_date_along_slope' , r.icesat2_slope_data.mean_along.dropna().index.min() if 'mean_along' in r.icesat2_slope_data else np.nan)\n",
    "print('min_date_combined_slope' , mixed_slope_series.index.min() if mixed_slope_series is not None else np.nan)\n",
    "print('max_date_across_slope' , r.icesat2_slope_data.mean_across.dropna().index.max() if 'mean_across' in r.icesat2_slope_data else np.nan)\n",
    "print('max_date_along_slope' , r.icesat2_slope_data.mean_along.dropna().index.max() if 'mean_along' in r.icesat2_slope_data else np.nan)\n",
    "print('max_date_combined_slope' , mixed_slope_series.index.max() if mixed_slope_series is not None else np.nan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
